"""
    Activation functions of artificial neural networks implemented from scratch

        - Linear
        - Sigmoid or Logistic
        - Hyperbolic Tangent
        - Rectified Linear Unit (ReLu)
        - Leaky ReLu
        - Scaled Exponential Linear Unit (SELU)
        - Softplus
        - Softmax
"""
